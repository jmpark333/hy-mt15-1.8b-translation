<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HY-MT1.5-1.8B: í…ì„¼íŠ¸ í˜¼ìœ„ì•ˆì˜ ê²½ëŸ‰ ë²ˆì—­ ëª¨ë¸ ì™„ë²½ ê°€ì´ë“œ</title>
    <meta name="description" content="í…ì„¼íŠ¸ í˜¼ìœ„ì•ˆ HY-MT1.5-1.8B ëª¨ë¸ ì†Œê°œ, ì„¤ì¹˜ ë°©ë²•, ì‹¤ì „ ì‚¬ìš© ì˜ˆì œê¹Œì§€ ì™„ë²½ ì •ë¦¬. 33ê°œ ì–¸ì–´ ì§€ì› ê²½ëŸ‰ ë²ˆì—­ ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ì„¸ìš”.">
    <meta property="og:title" content="HY-MT1.5-1.8B: í…ì„¼íŠ¸ í˜¼ìœ„ì•ˆì˜ ê²½ëŸ‰ ë²ˆì—­ ëª¨ë¸ ì™„ë²½ ê°€ì´ë“œ">
    <meta property="og:description" content="1.8B íŒŒë¼ë¯¸í„°ë¡œ 33ê°œ ì–¸ì–´ ë²ˆì—­ ê°€ëŠ¥! í…ì„¼íŠ¸ì˜ HY-MT1.5-1.8Bë¥¼ ì„¤ì¹˜í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.">
    <meta property="og:type" content="article">
</head>
<body>
<div>
<style>
/* AI ê¸°ìˆ  ë¸”ë¡œê·¸ í‘œì¤€ ìŠ¤íƒ€ì¼ */
body {
  font-family: "Noto Sans KR", "Segoe UI", sans-serif;
  font-size: 17px;
  line-height: 1.8;
  color: #333;
}
p {
    margin-bottom: 1.2em;
    word-break: keep-all;
}
h2 {
  border-bottom: 2px solid #0066cc;
  padding-bottom: 10px;
  margin-top: 40px;
  margin-bottom: 20px;
  font-weight: bold;
  color: #0066cc;
}
h3 {
    font-size: 1.2em;
    margin-top: 30px;
    margin-bottom: 15px;
    border-bottom: 1px solid #e6f2ff;
    padding-bottom: 5px;
    color: #0066cc;
}
.concept-block {
    background-color: #f0f8ff;
    border-left: 4px solid #0066cc;
    padding: 15px 20px;
    margin: 20px 0;
    border-radius: 0 8px 8px 0;
}
.code-block {
    background-color: #f5f5f5;
    border: 1px solid #ddd;
    border-radius: 8px;
    padding: 15px;
    margin: 20px 0;
    font-family: 'Courier New', monospace;
    position: relative;
    overflow: hidden;
    transition: all 0.3s ease;
}
.performance-table {
  width: 100%;
  border-collapse: collapse;
  margin: 20px 0;
  font-size: 16px;
  text-align: center;
}
.performance-table th, .performance-table td {
  border: 1px solid #ddd;
  padding: 12px;
}
.performance-table th {
  background-color: #0066cc;
  color: white;
  font-weight: bold;
}
.performance-table tr:nth-child(even) {
  background-color: #f2f2f2;
}
.ai-tech {
  background-color: #e6f3ff;
  padding: 3px 8px;
  border-radius: 4px;
  font-weight: bold;
  color: #0066cc;
}
.warning {
    background-color: #fff3cd;
    border-left: 5px solid #ffc107;
    padding: 15px;
    margin: 20px 0;
}
.info {
    background-color: #d1ecf1;
    border-left: 5px solid #17a2b8;
    padding: 15px;
    margin: 20px 0;
}
.code-block.collapsed {
    max-height: 200px;
}
.code-block.expanded {
    max-height: none;
}
.code-toggle-btn {
    position: absolute;
    bottom: 10px;
    right: 10px;
    background-color: rgba(0, 102, 204, 0.8);
    color: white;
    border: none;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
    font-size: 12px;
    z-index: 10;
    transition: background-color 0.3s ease;
}
.code-toggle-btn:hover {
    background-color: rgba(0, 102, 204, 1);
}
.code-block.collapsed .code-toggle-btn::after {
    content: "í¼ì¹˜ê¸° â–¼";
}
.code-block.expanded .code-toggle-btn::after {
    content: "ì ‘ê¸° â–²";
}
.code-fade-overlay {
    position: absolute;
    bottom: 0;
    left: 0;
    right: 0;
    height: 50px;
    background: linear-gradient(transparent, #f5f5f5);
    pointer-events: none;
    opacity: 1;
    transition: opacity 0.3s ease;
}
.code-block.expanded .code-fade-overlay {
    opacity: 0;
}
</style>
</div>

<p data-ke-size="size16">ìµœê·¼ AI ë²ˆì—­ ê¸°ìˆ ì´ ê¸‰ê²©íˆ ë°œì „í•˜ë©´ì„œ, <span class="ai-tech">ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)</span> ê¸°ë°˜ì˜ ë²ˆì—­ ì‹œìŠ¤í…œì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. í…ì„¼íŠ¸(Tencent)ì—ì„œ ê°œë°œí•œ <b>í˜¼ìœ„ì•ˆ(Hunyuan) ë²ˆì—­ ëª¨ë¸ HY-MT1.5-1.8B</b>ëŠ” ê²½ëŸ‰í™”ëœ í¬ê¸°ì—ë„ ë¶ˆêµ¬í•˜ê³  33ê°œ ì–¸ì–´ë¥¼ ìƒí˜¸ ë²ˆì—­í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ìë‘í•©ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” HY-MT1.5-1.8B ëª¨ë¸ì˜ íŠ¹ì§•ê³¼ ì‹¤ì œ ì‚¬ìš© ë°©ë²•ì„ ìƒì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>

<h2 id="introduction" data-ke-size="size23">ğŸ¤– HY-MT1.5-1.8B ì†Œê°œ</h2>

<p data-ke-size="size16">HY-MT1.5-1.8BëŠ” í…ì„¼íŠ¸ í˜¼ìœ„ì•ˆ íŒ€ì—ì„œ ê°œë°œí•œ <b>ë²ˆì—­ ì „ìš© AI ëª¨ë¸</b>ì…ë‹ˆë‹¤. ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ì´ ëª¨ë¸ì€ ì•½ <b>18ì–µ ê°œ(1.8B)</b>ì˜ íŒŒë¼ë¯¸í„°(ë§¤ê°œë³€ìˆ˜)ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ëŠ” AI ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ê°€ì¤‘ì¹˜ ê°’ë“¤ë¡œ, ê°œìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ë³µì¡ë„ì™€ ì„±ëŠ¥ì´ ë†’ì•„ì§€ì§€ë§Œ ê·¸ë§Œí¼ ë” ë§ì€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤.</p>

<div class="concept-block">
<p data-ke-size="size16"><b>ğŸ“Œ íŒŒë¼ë¯¸í„°(Parameter)ë€?</b><br>
AI ëª¨ë¸ì—ì„œ íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì—ì„œ í•™ìŠµí•œ ëª¨ë“  ì§€ì‹ì„ ì €ì¥í•˜ëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤. ì‰½ê²Œ ë¹„ìœ í•˜ìë©´, íŒŒë¼ë¯¸í„°ëŠ” ì¸ê°„ì˜ ë‡Œì„¸í¬ì²˜ëŸ¼ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  ì €ì¥í•˜ëŠ” ë‹¨ìœ„ì…ë‹ˆë‹¤. 1.8B íŒŒë¼ë¯¸í„°ëŠ” ì•½ 18ì–µ ê°œì˜ ì§€ì‹ ì €ì¥ì†Œê°€ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.</p>
</div>

<p data-ke-size="size16"><b>í•µì‹¬ íŠ¹ì§•:</b></p>
<ul>
<li>ê²½ëŸ‰í™”ëœ í¬ê¸°ë¡œë„ <span class="ai-tech">7B ëª¨ë¸</b>ê³¼ ìœ ì‚¬í•œ ë²ˆì—­ ì„±ëŠ¥ ë‹¬ì„±</li>
<li>33ê°œ ì–¸ì–´ ìƒí˜¸ ë²ˆì—­ ì§€ì› (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“±)</li>
<li>5ê°œ ì†Œìˆ˜ ì–¸ì–´ ë° ë°©ì–¸ ì§€ì› (ê´‘ë™ì–´, ìœ„êµ¬ë¥´ì–´, í‹°ë² íŠ¸ì–´ ë“±)</li>
<li>ì—£ì§€ ë””ë°”ì´ìŠ¤(Edge Device) ë°°í¬ ê°€ëŠ¥</li>
<li>ì‹¤ì‹œê°„ ë²ˆì—­ ì‹œë‚˜ë¦¬ì˜¤ì— ìµœì í™”</li>
<li>ìš©ì–´ ê°œì…(Terminology Intervention), ë¬¸ë§¥ ë²ˆì—­, í˜•ì‹ ë³´ì¡´ ë²ˆì—­ ì§€ì›</li>
</ul>

<h2 id="why-hy-mt15" data-ke-size="size23">ğŸ’¡ HY-MT1.5-1.8Bê°€ íŠ¹ë³„í•œ ì´ìœ </h2>

<p data-ke-size="size16">HY-MT1.5-1.8BëŠ” ë‹¨ìˆœí•œ ë²ˆì—­ ëª¨ë¸ì´ ì•„ë‹™ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ ê°€ì¥ í° ì¥ì ì€ <b>"ì‘ì§€ë§Œ ê°•ë ¥í•˜ë‹¤"</b>ëŠ” ì ì…ë‹ˆë‹¤.</p>

<h3 id="performance-efficiency" data-ke-size="size20">âš¡ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì˜ ê· í˜•</h3>

<p data-ke-size="size16">HY-MT1.5-7B ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° í¬ê¸°ê°€ 3ë¶„ì˜ 1 ë¯¸ë§Œì´ì§€ë§Œ, ë²ˆì—­ ì„±ëŠ¥ì€ ê±°ì˜ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì¥ì ì…ë‹ˆë‹¤.</p>

<div class="concept-block">
<p data-ke-size="size16"><b>ğŸ¯ ì™œ ê²½ëŸ‰ ëª¨ë¸ì´ ì¤‘ìš”í• ê¹Œìš”?</b></p>
<p data-ke-size="size16">ê²½ëŸ‰ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì´ ìˆìŠµë‹ˆë‹¤:</p>
<ul>
<li><b>ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©:</b> ì¼ë°˜ PCë‚˜ ë…¸íŠ¸ë¶ì—ì„œë„ êµ¬ë™ ê°€ëŠ¥</li>
<li><b>ë¹ ë¥¸ ì¶”ë¡  ì†ë„:</b> ì‹¤ì‹œê°„ ë²ˆì—­ ì„œë¹„ìŠ¤ ì œê³µì— ì í•©</li>
<li><b>ë‚®ì€ ìš´ì˜ ë¹„ìš©:</b> í´ë¼ìš°ë“œ ì„œë²„ ë¹„ìš© ì ˆê°</li>
<li><b>ì—£ì§€ ë””ë°”ì´ìŠ¤ ë°°í¬:</b> ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿ ë“±ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥</li>
</ul>
</div>

<h3 id="advanced-features" data-ke-size="size20">ğŸ”§ ê³ ê¸‰ ë²ˆì—­ ê¸°ëŠ¥</h3>

<p data-ke-size="size16">HY-MT1.5 ì‹œë¦¬ì¦ˆëŠ” ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ë²ˆì—­ì„ ë„˜ì–´ ë‹¤ìŒê³¼ ê°™ì€ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤:</p>

<p data-ke-size="size16"><b>1. ìš©ì–´ ê°œì… (Terminology Intervention)</b><br>
íŠ¹ì • ì „ë¬¸ ìš©ì–´ë‚˜ ê³ ìœ ëª…ì‚¬ê°€ ì›í•˜ëŠ” ëŒ€ë¡œ ë²ˆì—­ë˜ë„ë¡ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "Apple"ì„ "ì‚¬ê³¼" ëŒ€ì‹  "ì• í”Œ(ê¸°ì—…)"ë¡œ ë²ˆì—­í•˜ë„ë¡ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p data-ke-size="size16"><b>2. ë¬¸ë§¥ ë²ˆì—­ (Contextual Translation)</b><br>
ì´ì „ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë” ì •í™•í•œ ë²ˆì—­ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "bank"ê°€ "ì€í–‰"ì¸ì§€ "ê°•ê°€"ì¸ì§€ ë¬¸ë§¥ì— ë”°ë¼ êµ¬ë¶„í•©ë‹ˆë‹¤.</p>

<p data-ke-size="size16"><b>3. í˜•ì‹ ë³´ì¡´ ë²ˆì—­ (Formatted Translation)</b><br>
HTML íƒœê·¸, ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë“±ì„ ë³´ì¡´í•˜ë©´ì„œ ë²ˆì—­í•©ë‹ˆë‹¤. ì›¹ì‚¬ì´íŠ¸ ë‹¤êµ­ì–´í™” ì‘ì—…ì— ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.</p>

<h2 id="supported-languages" data-ke-size="size23">ğŸŒ ì§€ì› ì–¸ì–´</h2>

<p data-ke-size="size16">HY-MT1.5-1.8BëŠ” ì´ 33ê°œ ì–¸ì–´ì™€ 5ê°œ ì†Œìˆ˜ ì–¸ì–´/ë°©ì–¸ì„ ì§€ì›í•©ë‹ˆë‹¤. ì£¼ìš” ì§€ì› ì–¸ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>

<table class="performance-table">
<thead>
<tr>
<th>ì–¸ì–´</th>
<th>ì•½ì–´</th>
<th>ì–¸ì–´</th>
<th>ì•½ì–´</th>
</tr>
</thead>
<tbody>
<tr>
<td>í•œêµ­ì–´</td>
<td>ko</td>
<td>ì˜ì–´</td>
<td>en</td>
</tr>
<tr>
<td>ì¼ë³¸ì–´</td>
<td>ja</td>
<td>ì¤‘êµ­ì–´(ê°„ì²´)</td>
<td>zh</td>
</tr>
<tr>
<td>ì¤‘êµ­ì–´(ë²ˆì²´)</td>
<td>zh-Hant</td>
<td>í”„ë‘ìŠ¤ì–´</td>
<td>fr</td>
</tr>
<tr>
<td>ìŠ¤í˜ì¸ì–´</td>
<td>es</td>
<td>í¬ë¥´íˆ¬ê°ˆì–´</td>
<td>pt</td>
</tr>
<tr>
<td>ë…ì¼ì–´</td>
<td>de</td>
<td>ì´íƒˆë¦¬ì•„ì–´</td>
<td>it</td>
</tr>
<tr>
<td>ëŸ¬ì‹œì•„ì–´</td>
<td>ru</td>
<td>ì•„ëì–´</td>
<td>ar</td>
</tr>
<tr>
<td>íŒë””ì–´</td>
<td>hi</td>
<td>íƒœêµ­ì–´</td>
<td>th</td>
</tr>
<tr>
<td>ë² íŠ¸ë‚¨ì–´</td>
<td>vi</td>
<td>ì¸ë„ë„¤ì‹œì•„ì–´</td>
<td>id</td>
</tr>
<tr>
<td>ë§ë ˆì´ì–´</td>
<td>ms</td>
<td>í„°í‚¤ì–´</td>
<td>tr</td>
</tr>
</tbody>
</table>

<p data-ke-size="size16"><b>ì†Œìˆ˜ ì–¸ì–´/ë°©ì–¸:</b> ê´‘ë™ì–´, í‹°ë² íŠ¸ì–´, ì¹´ìíì–´, ëª½ê³¨ì–´, ìœ„êµ¬ë¥´ì–´ ë“±</p>

<h2 id="installation" data-ke-size="size23">ğŸš€ ì„¤ì¹˜ ë°©ë²•</h2>

<p data-ke-size="size16">ì´ì œ HY-MT1.5-1.8Bë¥¼ ì§ì ‘ ì„¤ì¹˜í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. Python í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆ ì§€ì‹ì´ ìˆë‹¤ë©´ ë”°ë¼ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<h3 id="requirements" data-ke-size="size20">ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­</h3>

<div class="info">
<p data-ke-size="size16"><b>âš™ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­</b></p>
<ul>
<li><b>CPU:</b> ìµœì‹  ë©€í‹°ì½”ì–´ í”„ë¡œì„¸ì„œ</li>
<li><b>RAM:</b> ìµœì†Œ 8GB (ê¶Œì¥ 16GB ì´ìƒ)</li>
<li><b>GPU:</b> NVIDIA GPU (ê¶Œì¥) ë˜ëŠ” CPU-only ê°€ëŠ¥</li>
<li><b>ìš´ì˜ì²´ì œ:</b> Linux, macOS, Windows</li>
<li><b>Python:</b> 3.8 ì´ìƒ</li>
</ul>
</div>

<h3 id="transformers-install" data-ke-size="size20">ğŸ”§ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì„¤ì¹˜í•˜ê¸°</h3>

<p data-ke-size="size16">ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í„°ë¯¸ë„(ë˜ëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸)ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:</p>

<div class="code-block">
<pre class="bash"><code># Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ë²„ì „ 4.56.0 ê¶Œì¥)
pip install transformers==4.56.0

# ì¶”ê°€ í•„ìš” íŒ¨í‚¤ì§€
pip install torch accelerate sentencepiece</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<div class="warning">
<p data-ke-size="size16"><b>âš ï¸ ì£¼ì˜ì‚¬í•­</b><br>
GPUê°€ ì—†ëŠ” ê²½ìš° CPU-onlyë¡œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ì§€ë§Œ ì†ë„ê°€ ëŠë ¤ì§‘ë‹ˆë‹¤. NVIDIA GPUê°€ ìˆëŠ” ê²½ìš° CUDA ë²„ì „ì— ë§ëŠ” PyTorchë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.</p>
</div>

<h3 id="basic-usage" data-ke-size="size20">ğŸ’» ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ</h3>

<p data-ke-size="size16">ì´ì œ Python ì½”ë“œë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ë²ˆì—­ì„ ìˆ˜í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤:</p>

<div class="code-block">
<pre class="python"><code>from transformers import AutoModelForCausalLM, AutoTokenizer

# ëª¨ë¸ ì´ë¦„ ì§€ì •
model_name = "tencent/HY-MT1.5-1.8B"

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
print("ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
    torch_dtype="auto"  # ì ì ˆí•œ ë°ì´í„° íƒ€ì… ìë™ ì„ íƒ
)

# ë²ˆì—­í•  í…ìŠ¤íŠ¸ì™€ ëŒ€ìƒ ì–¸ì–´ ì„¤ì •
source_text = "Hello, how are you today?"
target_language = "Korean"

# í”„ë¡¬í”„íŠ¸ ìƒì„±
messages = [
    {
        "role": "user",
        "content": f"Translate the following segment into {target_language}, without additional explanation.\n\n{source_text}"
    }
]

# í† í°í™”
tokenized_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors="pt"
)

# ë²ˆì—­ ìƒì„±
print("ë²ˆì—­ ì¤‘...")
outputs = model.generate(
    tokenized_chat.to(model.device),
    max_new_tokens=2048,
    temperature=0.7,
    top_p=0.6,
    top_k=20,
    repetition_penalty=1.05
)

# ê²°ê³¼ ë””ì½”ë”©
output_text = tokenizer.decode(outputs[0])
print("\në²ˆì—­ ê²°ê³¼:")
print(output_text)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<h3 id="korean-translation" data-ke-size="size20">ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë²ˆì—­ ì˜ˆì œ</h3>

<p data-ke-size="size16">í•œêµ­ì–´ì™€ ì˜ì–´ ê°„ ë²ˆì—­ì€ ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:</p>

<div class="code-block">
<pre class="python"><code># ì˜ì–´ â†’ í•œêµ­ì–´
def translate_to_korean(text):
    messages = [
        {
            "role": "user",
            "content": f"Translate the following segment into Korean, without additional explanation.\n\n{text}"
        }
    ]

    tokenized_chat = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        return_tensors="pt"
    )

    outputs = model.generate(
        tokenized_chat.to(model.device),
        max_new_tokens=2048,
        temperature=0.7,
        top_p=0.6,
        top_k=20,
        repetition_penalty=1.05
    )

    return tokenizer.decode(outputs[0])

# ì‚¬ìš© ì˜ˆì‹œ
english_text = "Artificial intelligence is transforming the world."
korean_result = translate_to_korean(english_text)
print("ë²ˆì—­ ê²°ê³¼:", korean_result)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<div class="code-block">
<pre class="python"><code># í•œêµ­ì–´ â†’ ì˜ì–´
def translate_to_english(text):
    messages = [
        {
            "role": "user",
            "content": f"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸ºè‹±è¯­ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š\n\n{text}"
        }
    ]

    tokenized_chat = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        return_tensors="pt"
    )

    outputs = model.generate(
        tokenized_chat.to(model.device),
        max_new_tokens=2048,
        temperature=0.7,
        top_p=0.6,
        top_k=20,
        repetition_penalty=1.05
    )

    return tokenizer.decode(outputs[0])

# ì‚¬ìš© ì˜ˆì‹œ
korean_text = "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ìš°ë¦¬ ì‚¶ì„ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤."
english_result = translate_to_english(korean_text)
print("Translation:", english_result)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<h3 id="terminology-intervention" data-ke-size="size20">ğŸ¯ ìš©ì–´ ê°œì… ë²ˆì—­ ì˜ˆì œ</h3>

<p data-ke-size="size16">íŠ¹ì • ìš©ì–´ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë²ˆì—­í•˜ë„ë¡ ì œì–´í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:</p>

<div class="code-block">
<pre class="python"><code>def translate_with_terminology(source_text, source_term, target_term, target_language):
    """
    ìš©ì–´ ê°œì… ë²ˆì—­ í•¨ìˆ˜

    Args:
        source_text: ë²ˆì—­í•  ì›ë³¸ í…ìŠ¤íŠ¸
        source_term: ì›ë³¸ ìš©ì–´
        target_term: ë²ˆì—­ë  ìš©ì–´
        target_language: ëŒ€ìƒ ì–¸ì–´
    """
    if target_language == "Chinese":
        prompt = f"""å‚è€ƒä¸‹é¢çš„ç¿»è¯‘ï¼š
{source_term} ç¿»è¯‘æˆ {target_term}

å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š
{source_text}"""
    else:
        prompt = f"""Reference translation:
{source_term} should be translated as {target_term}

Translate the following segment into {target_language}, without additional explanation.
{source_text}"""

    messages = [{"role": "user", "content": prompt}]

    tokenized_chat = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        return_tensors="pt"
    )

    outputs = model.generate(
        tokenized_chat.to(model.device),
        max_new_tokens=2048,
        temperature=0.7,
        top_p=0.6,
        top_k=20,
        repetition_penalty=1.05
    )

    return tokenizer.decode(outputs[0])

# ì‚¬ìš© ì˜ˆì‹œ: "Cloud"ë¥¼ "í´ë¼ìš°ë“œ"ë¡œ ì¼ê´€ë˜ê²Œ ë²ˆì—­
text = "The company provides Cloud services for enterprise customers."
result = translate_with_terminology(
    source_text=text,
    source_term="Cloud",
    target_term="í´ë¼ìš°ë“œ",
    target_language="Korean"
)
print("ë²ˆì—­ ê²°ê³¼:", result)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<h2 id="ollama-installation" data-ke-size="size23">ğŸ¦™ Ollamaë¡œ ë¡œì»¬ ì„¤ì¹˜í•˜ê¸°</h2>

<p data-ke-size="size16">Python ì½”ë“œë¥¼ ì‘ì„±í•˜ì§€ ì•Šê³  ë” ì‰½ê²Œ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´, <b>Ollama</b>ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. HY-MT1.5-1.8BëŠ” GGUF í¬ë§·ìœ¼ë¡œ ì œê³µë˜ì–´ Ollamaì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.</p>

<h3 id="ollama-setup" data-ke-size="size20">ğŸ“¥ Ollama ì„¤ì¹˜ ë° ì„¤ì •</h3>

<div class="code-block">
<pre class="bash"><code># 1. Ollama ì„¤ì¹˜ (Linux/macOS)
curl -fsSL https://ollama.com/install.sh | sh

# Windowsì˜ ê²½ìš°: https://ollama.com/downloadì—ì„œ ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
ollama run tencent-hy-mt15-1.8b

# 3. ë²ˆì—­ í…ŒìŠ¤íŠ¸
echo "Translate this to Korean: Hello, world" | ollama run tencent-hy-mt15-1.8b</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<p data-ke-size="size16">Ollamaë¥¼ ì‚¬ìš©í•˜ë©´ ëª…ë ¹ì¤„ì—ì„œ ë°”ë¡œ ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ ê°„í¸í•©ë‹ˆë‹¤. ë˜í•œ OllamaëŠ” OpenAI í˜¸í™˜ APIë„ ì œê³µí•˜ë¯€ë¡œ ê¸°ì¡´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<h3 id="ollama-python-api" data-ke-size="size20">ğŸ Ollama Python API ì‚¬ìš©</h3>

<div class="code-block">
<pre class="python"><code>import ollama

# Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = ollama.Client()

# ë²ˆì—­ í•¨ìˆ˜
def translate_with_ollama(text, target_lang="Korean"):
    prompt = f"Translate the following segment into {target_lang}, without additional explanation.\n\n{text}"

    response = client.chat(
        model='tencent-hy-mt15-1.8b',
        messages=[{'role': 'user', 'content': prompt}],
        options={
            'temperature': 0.7,
            'top_p': 0.6,
            'top_k': 20,
            'num_predict': 2048
        }
    )

    return response['message']['content']

# ì‚¬ìš© ì˜ˆì‹œ
result = translate_with_ollama(
    "Machine learning is a subset of artificial intelligence.",
    "Korean"
)
print("ë²ˆì—­ ê²°ê³¼:", result)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<h2 id="practical-examples" data-ke-size="size23">ğŸ’¼ ì‹¤ì „ í™œìš© ì˜ˆì œ</h2>

<p data-ke-size="size16">ì´ì œ ì‹¤ì œ ì—…ë¬´ë‚˜ í”„ë¡œì íŠ¸ì—ì„œ HY-MT1.5-1.8Bë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.</p>

<h3 id="batch-translation" data-ke-size="size20">ğŸ“„ ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ íŒŒì¼ ë²ˆì—­</h3>

<div class="code-block">
<pre class="python"><code>def translate_file(input_file, output_file, target_language):
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ ë²ˆì—­ í›„ ì €ì¥

    Args:
        input_file: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        target_language: ëŒ€ìƒ ì–¸ì–´
    """
    # íŒŒì¼ ì½ê¸°
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()

    print(f"íŒŒì¼ ë²ˆì—­ ì‹œì‘: {input_file}")
    print(f"ëŒ€ìƒ ì–¸ì–´: {target_language}")

    # ë²ˆì—­ ìˆ˜í–‰
    messages = [
        {
            "role": "user",
            "content": f"Translate the following segment into {target_language}, without additional explanation.\n\n{content}"
        }
    ]

    tokenized_chat = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        return_tensors="pt"
    )

    outputs = model.generate(
        tokenized_chat.to(model.device),
        max_new_tokens=4096,
        temperature=0.7,
        top_p=0.6,
        top_k=20,
        repetition_penalty=1.05
    )

    translated_text = tokenizer.decode(outputs[0])

    # íŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(translated_text)

    print(f"ë²ˆì—­ ì™„ë£Œ! ì €ì¥ë¨: {output_file}")

# ì‚¬ìš© ì˜ˆì‹œ
translate_file(
    input_file="document_en.txt",
    output_file="document_ko.txt",
    target_language="Korean"
)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<h3 id="web-api" data-ke-size="size20">ğŸŒ ê°„ë‹¨í•œ ë²ˆì—­ API ì„œë²„ ë§Œë“¤ê¸°</h3>

<p data-ke-size="size16">Flaskë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ë²ˆì—­ API ì„œë²„ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:</p>

<div class="code-block">
<pre class="python"><code>from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer

app = Flask(__name__)

# ëª¨ë¸ ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
model_name = "tencent/HY-MT1.5-1.8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)

@app.route('/translate', methods=['POST'])
def translate():
    """
    ë²ˆì—­ API ì—”ë“œí¬ì¸íŠ¸

    JSON í˜•ì‹:
    {
        "text": "ë²ˆì—­í•  í…ìŠ¤íŠ¸",
        "target_language": "Korean"
    }
    """
    try:
        data = request.get_json()
        text = data.get('text', '')
        target_language = data.get('target_language', 'Korean')

        if not text:
            return jsonify({'error': 'í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'}), 400

        # ë²ˆì—­ ìˆ˜í–‰
        messages = [
            {
                "role": "user",
                "content": f"Translate the following segment into {target_language}, without additional explanation.\n\n{text}"
            }
        ]

        tokenized_chat = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=False,
            return_tensors="pt"
        )

        outputs = model.generate(
            tokenized_chat.to(model.device),
            max_new_tokens=2048,
            temperature=0.7,
            top_p=0.6,
            top_k=20,
            repetition_penalty=1.05
        )

        translated_text = tokenizer.decode(outputs[0])

        return jsonify({
            'success': True,
            'original_text': text,
            'translated_text': translated_text,
            'target_language': target_language
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("ë²ˆì—­ API ì„œë²„ ì‹œì‘ä¸­...")
    app.run(host='0.0.0.0', port=5000, debug=True)</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<p data-ke-size="size16"><b>API ì‚¬ìš© ì˜ˆì‹œ (curl):</b></p>

<div class="code-block">
<pre class="bash"><code>curl -X POST http://localhost:5000/translate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, how are you?",
    "target_language": "Korean"
  }'</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<h2 id="quantization" data-ke-size="size23">âš¡ ì–‘ìí™”(Quantization)ë¡œ ì†ë„ ë†’ì´ê¸°</h2>

<p data-ke-size="size16">HY-MT1.5-1.8BëŠ” ì–‘ìí™”ëœ ë²„ì „ë„ ì œê³µí•©ë‹ˆë‹¤. ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ì •ë°€ë„ë¥¼ ë‚®ì¶”ì–´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì¶”ë¡  ì†ë„ë¥¼ ê°œì„ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.</p>

<div class="concept-block">
<p data-ke-size="size16"><b>ğŸ“Œ ì–‘ìí™”(Quantization)ë€?</b><br>
ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ê°’ì„ 16ë¹„íŠ¸ë‚˜ 32ë¹„íŠ¸ì—ì„œ 8ë¹„íŠ¸ë‚˜ 4ë¹„íŠ¸ë¡œ ì¤„ì´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì‰½ê²Œ ë§í•´, ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¥¼ ì••ì¶•í•˜ì—¬ ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤. ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ëª¨ë¸ í¬ê¸°ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
</div>

<h3 id="int4-quantization" data-ke-size="size20">ğŸ¯ INT4 ì–‘ìí™” ëª¨ë¸ ì‚¬ìš©</h3>

<p data-ke-size="size16">INT4 ì–‘ìí™” ë²„ì „ì„ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë” ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>

<div class="code-block">
<pre class="python"><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# INT4 ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
model_name = "tencent/HY-MT1.5-1.8B-GPTQ-Int4"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

# ì´í›„ ì‚¬ìš© ë°©ë²•ì€ ë™ì¼
messages = [
    {
        "role": "user",
        "content": "Translate the following segment into Korean, without additional explanation.\n\nHello, world!"
    }
]

tokenized_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors="pt"
)

outputs = model.generate(
    tokenized_chat.to(model.device),
    max_new_tokens=2048
)

print(tokenizer.decode(outputs[0]))</code></pre>
<button class="code-toggle-btn"></button>
<div class="code-fade-overlay">&nbsp;</div>
</div>

<div class="info">
<p data-ke-size="size16"><b>ğŸ’¡ ì–‘ìí™” ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ</b></p>
<ul>
<li><b>FP8:</b> ì„±ëŠ¥ ì €í•˜ ìµœì†Œ, GPU ê¶Œì¥</li>
<li><b>INT4:</b> ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½, CPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥</li>
<li><b>GGUF:</b> Ollama ë“± ë¡œì»¬ í™˜ê²½ì—ì„œ ì‚¬ìš©</li>
</ul>
</div>

<h2 id="comparison" data-ke-size="size23">ğŸ“Š ë‹¤ë¥¸ ë²ˆì—­ ëª¨ë¸ê³¼ ë¹„êµ</h2>

<p data-ke-size="size16">HY-MT1.5-1.8Bë¥¼ ë‹¤ë¥¸ ì¸ê¸° ë²ˆì—­ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•´ ë³´ê² ìŠµë‹ˆë‹¤:</p>

<table class="performance-table">
<thead>
<tr>
<th>ëª¨ë¸</th>
<th>íŒŒë¼ë¯¸í„°</th>
<th>ì§€ì› ì–¸ì–´</th>
<th>ì¥ì </th>
<th>ë‹¨ì </th>
</tr>
</thead>
<tbody>
<tr>
<td>HY-MT1.5-1.8B</td>
<td>1.8B</td>
<td>33+5</td>
<td>ê²½ëŸ‰, ê³ ì„±ëŠ¥, ë‹¤êµ­ì–´</td>
<td>ìƒëŒ€ì ìœ¼ë¡œ ì‹ ê·œ ëª¨ë¸</td>
</tr>
<tr>
<td>NLLB-200</td>
<td>3.3B/1.3B</td>
<td>200</td>
<td>ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›</td>
<td>ìš©ì–´ ê°œì… ê¸°ëŠ¥ ë¶€ì¡±</td>
</tr>
<tr>
<td>SeamlessM4T</td>
<td>2.3B</td>
<td>100</td>
<td>ìŒì„± ë²ˆì—­ ì§€ì›</td>
<td>í…ìŠ¤íŠ¸ ì „ë¬¸ ì•„ë‹˜</td>
</tr>
<tr>
<td>Google Translate API</td>
<td>-</td>
<td>130+</td>
<td>ì•ˆì •ì , ì‹ ë¢°ì„±</td>
<td>ìœ ë£Œ, API ì˜ì¡´</td>
</tr>
<tr>
<td>DeepL API</td>
<td>-</td>
<td>30</td>
<td>ë²ˆì—­ í’ˆì§ˆ ìš°ìˆ˜</td>
<td>ìœ ë£Œ, ì–¸ì–´ ì œí•œ</td>
</tr>
</tbody>
</table>

<p data-ke-size="size16"><b>HY-MT1.5-1.8Bì˜ ê²½ìŸë ¥:</b></p>
<ul>
<li>ì˜¤í”ˆì†ŒìŠ¤ë¡œ ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥</li>
<li>ë¡œì»¬ ë°°í¬ë¡œ ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´í˜¸</li>
<li>ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥ (íŒŒì¸íŠœë‹)</li>
<li>ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ ë²ˆì—­ ê°€ëŠ¥</li>
<li>ìš©ì–´ ê°œì…, ë¬¸ë§¥ ë²ˆì—­ ë“± ê³ ê¸‰ ê¸°ëŠ¥ ì§€ì›</li>
</ul>

<h2 id="limitations" data-ke-size="size23">âš ï¸ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­</h2>

<p data-ke-size="size16">HY-MT1.5-1.8BëŠ” ê°•ë ¥í•œ ë²ˆì—­ ëª¨ë¸ì´ì§€ë§Œ, ë‹¤ìŒê³¼ ê°™ì€ ì œí•œì‚¬í•­ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:</p>

<ul>
<li><b>ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸:</b> í•œ ë²ˆì— ë²ˆì—­í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ê¸¸ì´ì— ì œí•œì´ ìˆìŒ</li>
<li><b>ì „ë¬¸ ë¶„ì•¼:</b> ì˜í•™, ë²•ë¥  ë“± ì „ë¬¸ ìš©ì–´ê°€ ë§ì€ ë¶„ì•¼ëŠ” ì¶”ê°€ íŒŒì¸íŠœë‹ í•„ìš”</li>
<li><b>ì‹¤ì‹œê°„ ì„±ëŠ¥:</b> CPU-only í™˜ê²½ì—ì„œëŠ” ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ</li>
<li><b>ì–¸ì–´ ê°„ ì„±ëŠ¥ ì°¨ì´:</b> ì£¼ìš” ì–¸ì–´(ì˜ì–´, ì¤‘êµ­ì–´)ì™€ ì†Œìˆ˜ ì–¸ì–´ ê°„ ì„±ëŠ¥ ì°¨ì´ ì¡´ì¬</li>
</ul>

<div class="warning">
<p data-ke-size="size16"><b>âš ï¸ ìƒì—…ì  ì‚¬ìš© ì£¼ì˜ì‚¬í•­</b><br>
HY-MT1.5-1.8BëŠ” ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì´ì§€ë§Œ, ìƒì—…ì  ì‚¬ìš© ì‹œ í…ì„¼íŠ¸ì˜ ë¼ì´ì„ ìŠ¤ ì •ì±…ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ë²ˆì—­ ê²°ê³¼ë¥¼ ìƒìš© ì„œë¹„ìŠ¤ì— ì œê³µí•  ê²½ìš° ë²•ì  ì±…ì„ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.</p>
</div>

<h2 id="conclusion" data-ke-size="size23">ğŸ¯ ë§ºìŒë§</h2>

<p data-ke-size="size16">HY-MT1.5-1.8BëŠ” í…ì„¼íŠ¸ í˜¼ìœ„ì•ˆ íŒ€ì´ ê°œë°œí•œ ê²½ëŸ‰ ë²ˆì—­ ëª¨ë¸ë¡œ, 33ê°œ ì–¸ì–´ë¥¼ ì§€ì›í•˜ë©´ì„œë„ ê²½ëŸ‰í™”ëœ í¬ê¸°ë¡œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. íŠ¹íˆ <b>ì—£ì§€ ë””ë°”ì´ìŠ¤ ë°°í¬</b>, <b>ì‹¤ì‹œê°„ ë²ˆì—­</b>, <b>ìš©ì–´ ê°œì…</b>, <b>ë¬¸ë§¥ ë²ˆì—­</b> ë“±ì˜ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ë‹¤ì–‘í•œ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p data-ke-size="size16">ì´ ê¸€ì—ì„œ ì†Œê°œí•œ ì„¤ì¹˜ ë°©ë²•ê³¼ ì½”ë“œ ì˜ˆì œë¥¼ ë”°ë¼ í•˜ì‹œë©´, ì—¬ëŸ¬ë¶„ë„ ê°œì¸ PCë‚˜ ë…¸íŠ¸ë¶ì—ì„œ ê°•ë ¥í•œ AI ë²ˆì—­ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ Ollamaë¥¼ í™œìš©í•˜ë©´ Python ì½”ë“œë¥¼ ì‘ì„±í•˜ì§€ ì•Šê³ ë„ ëª…ë ¹ì¤„ì—ì„œ ë°”ë¡œ ë²ˆì—­ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p data-ke-size="size16">HY-MT1.5-1.8B ëª¨ë¸ì€ <span class="ai-tech">ì˜¤í”ˆì†ŒìŠ¤</span>ë¡œ ë¬´ë£Œ ì œê³µë˜ë¯€ë¡œ, ë²ˆì—­ API ë¹„ìš©ì„ ì ˆê°í•˜ê³  ì‹¶ì€ ê¸°ì—…ì´ë‚˜ ê°œë°œìë“¤ì—ê²Œ í›Œë¥­í•œ ì„ íƒì§€ê°€ ë  ê²ƒì…ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ë„ í•œë²ˆ í…ì„¼íŠ¸ í˜¼ìœ„ì•ˆì˜ HY-MT1.5-1.8Bë¥¼ ì„¤ì¹˜í•´ì„œ ì§ì ‘ ì‚¬ìš©í•´ë³´ì‹œê¸¸ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.</p>

<p data-ke-size="size16">ê°ì‚¬í•©ë‹ˆë‹¤.</p>

<h2 id="references" data-ke-size="size23">ğŸ“š ì°¸ê³ ìë£Œ</h2>

<ul>
<li><a href="https://huggingface.co/tencent/HY-MT1.5-1.8B" target="_blank" rel="noopener">Hugging Face - HY-MT1.5-1.8B</a></li>
<li><a href="https://github.com/Tencent-Hunyuan/HY-MT" target="_blank" rel="noopener">GitHub - Tencent Hunyuan MT</a></li>
<li><a href="https://hunyuan.tencent.com/" target="_blank" rel="noopener">í˜¼ìœ„ì•ˆ ê³µì‹ ì›¹ì‚¬ì´íŠ¸</a></li>
<li><a href="https://ollama.com/" target="_blank" rel="noopener">Ollama ê³µì‹ ì›¹ì‚¬ì´íŠ¸</a></li>
<li><a href="https://arxiv.org/abs/2512.24092" target="_blank" rel="noopener">HY-MT1.5 Technical Report</a></li>
</ul>

<p data-ke-size="size16"><b>í‚¤ì›Œë“œ:</b> AI, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, LLM, ë²ˆì—­ ëª¨ë¸, í˜¼ìœ„ì•ˆ</p>

<p data-ke-size="size16">
<script>
document.addEventListener('DOMContentLoaded', function() {
    const codeBlocks = document.querySelectorAll('.code-block');

    codeBlocks.forEach(function(block) {
        const preElement = block.querySelector('pre');

        if (preElement) {
            const blockHeight = preElement.offsetHeight;

            if (blockHeight > 300) {
                block.classList.add('collapsed');

                const fadeOverlay = document.createElement('div');
                fadeOverlay.className = 'code-fade-overlay';
                block.appendChild(fadeOverlay);

                const toggleBtn = document.createElement('button');
                toggleBtn.className = 'code-toggle-btn';
                block.appendChild(toggleBtn);

                toggleBtn.addEventListener('click', function() {
                    toggleCodeBlock(block);
                });
            }
        }
    });

    function toggleCodeBlock(block) {
        const isCollapsed = block.classList.contains('collapsed');

        if (isCollapsed) {
            block.classList.remove('collapsed');
            block.classList.add('expanded');
        } else {
            block.classList.remove('expanded');
            block.classList.add('collapsed');
        }
    }
});
</script>
</p>

</body>
</html>